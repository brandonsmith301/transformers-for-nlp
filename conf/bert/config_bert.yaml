seed: 330

model:
    pretrained_model_path: "bert-base-uncased"
    max_seq_len: 512
    vocab_size: 30522
    n_layer: 12
    n_head: 12
    n_embd: 768
    dropout: 0.1
    bias: true
    num_classes: 2

dataset:
    tokenizer_path: "bert-base-uncased"
    use_fast: true
    padding_side: "right"
    truncation_side: "right"
    max_length: 512

training:
    learning_rate: 3e-5
    betas: [ 0.9, 0.999 ]
    weight_decay: 0.01
    batch_size: 4
    num_epochs: 10
    gradient_accumulation_steps: 4
    max_grad_norm: 1.0
    use_mask_aug: false
    mask_aug_prob: 0.15
    warmup_pct: 0.05
    save_trigger: 0.0
    eval_frequency: 100
    patience: 10

outputs:
    model_dir: outputs/bert/bert_base_uncased
